{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Na-ve-Bees_-Deep-Learning-with-Images.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMCPs+DvOiH2FAB+qV+vtZK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jhonn2021/Na-ve-Bees_-Deep-Learning-with-Images/blob/master/Na_ve_Bees__Deep_Learning_with_Images.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0l-S_wTgnBuK",
        "outputId": "90f3ccf2-d78f-4957-ddd2-0b82deec0f87",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "source": [
        "#from google.colab import files\n",
        "#files.upload()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-df228188-0368-4b75-983a-9480a5ae2f95\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-df228188-0368-4b75-983a-9480a5ae2f95\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving images_a.zip to images_a.zip\n",
            "Saving images_b.zip to images_b.zip\n",
            "Saving labels.csv to labels.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvRDP9pzoC8d"
      },
      "source": [
        "1. Import Python libraries\n",
        "honey beeA honey bee (Apis).\n",
        "\n",
        "Can a machine identify a bee as a honey bee or a bumble bee? These bees have different behaviors and appearances, but given the variety of backgrounds, positions, and image resolutions, it can be a challenge for machines to tell them apart.\n",
        "\n",
        "Being able to identify bee species from images is a task that ultimately would allow researchers to more quickly and effectively collect field data. Pollinating bees have critical roles in both ecology and agriculture, and diseases like colony collapse disorder threaten these species. Identifying different species of bees in the wild means that we can better understand the prevalence and growth of these important insects.\n",
        "\n",
        "bumble beeA bumble bee (Bombus).\n",
        "\n",
        "This notebook walks through building a simple deep learning model that can automatically detect honey bees and bumble bees and then loads a pre-trained model for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmKYBpOen72j"
      },
      "source": [
        "import pickle\n",
        "from pathlib import Path\n",
        "from skimage import io\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# import keras library\n",
        "# ... YOUR CODE FOR TASK 1 ...\n",
        "import keras\n",
        "\n",
        "# import Sequential from the keras models module\n",
        "# ... YOUR CODE FOR TASK 1 ...\n",
        "from keras.models import Sequential \n",
        "# import Dense, Dropout, Flatten, Conv2D, MaxPooling2D from the keras layers module\n",
        "from keras.layers import Dense , Dropout , Flatten , Conv2D , MaxPooling2D"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukIHQ0pfoPQN"
      },
      "source": [
        "2. Load image labels\n",
        "Now that we have all of our imports ready, it is time to look at the labels for our data. We will load our labels.csv file into a DataFrame called labels, where the index is the image name (e.g. an index of 1036 refers to an image named 1036.jpg) and the genus column tells us the bee type. genus takes the value of either 0.0 (Apis or honey bee) or 1.0 (Bombus or bumble bee)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XpAMnEeoQU9",
        "outputId": "02a5f978-d098-4555-de7f-507f5c6bdc52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# load labels.csv from datasets folder using pandas\n",
        "labels = pd.read_csv('labels.csv' , index_col = 0)\n",
        "\n",
        "# print value counts for genus\n",
        "print(pd.value_counts(labels.genus))\n",
        "\n",
        "# assign the genus label values to y\n",
        "y = np.array(labels['genus'])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0    827\n",
            "1.0    827\n",
            "Name: genus, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enW93BPko4Jf"
      },
      "source": [
        "3. Examine RGB values in an image matrix\n",
        "Image data can be represented as a matrix. The width of the matrix is the width of the image, the height of the matrix is the height of the image, and the depth of the matrix is the number of channels. Most image formats have three color channels: red, green, and blue.\n",
        "\n",
        "For each pixel in an image, there is a value for every channel. The combination of the three values corresponds to the color, as per the RGB color model. Values for each color can range from 0 to 255, so a purely blue pixel would show up as (0, 0, 255).\n",
        "\n",
        "\n",
        "\n",
        "Let's explore the data for a sample image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULbPU6LJo5pL"
      },
      "source": [
        "# load an image and explore\n",
        "example_image = io.imread('datasets/{}.jpg',format(labels.index[0]))\n",
        "\n",
        "# show image\n",
        "# ... YOUR CODE FOR TASK 3 ...\n",
        "plt.imshow(example_image)\n",
        "# print shape\n",
        "print('Example image has shape: ', example_image.shape)\n",
        "                    \n",
        "# print color channel values for top left pixel\n",
        "print('RGB values for the top left pixel are:', example_image[0,0,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXvmXMBpqzMI"
      },
      "source": [
        "4. Normalize image data\n",
        "Now we need to normalize our image data. Normalization is a general term that means changing the scale of our data so it is consistent.\n",
        "\n",
        "In this case, we want each feature to have a similar range so our neural network can learn effectively across all the features. As explained in the sklearn docs, \"If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.\"\n",
        "\n",
        "We will scale our data so that it has a mean of 0 and standard deviation of 1. We'll use sklearn's StandardScaler to do the math for us, which entails taking each value, subtracting the mean, and then dividing by the standard deviation. We need to do this for each color channel (i.e. each feature) individually."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8ycz_mVq23z"
      },
      "source": [
        "ss = StandardScaler()\n",
        "\n",
        "image_list = []\n",
        "for i in labels.index:\n",
        "    # load image\n",
        "    img = io.imread('datasets/{}.jpg'.format(i)).astype(np.float64)\n",
        "    \n",
        "    # for each channel, apply standard scaler's fit_transform method\n",
        "    for channel in range(img.shape[2]):\n",
        "        img[:, :, channel] = ss.fit_transform(img[:, :, channel])\n",
        "        \n",
        "    # append to list of all images\n",
        "    image_list.append(img)\n",
        "    \n",
        "# convert image list to single array\n",
        "X = np.array(image_list)\n",
        "\n",
        "# print shape of X\n",
        "print(X.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivyCXPcwq7fB"
      },
      "source": [
        "5. Split into train, test, and evaluation sets\n",
        "Now that we have our big image data matrix, X, as well as our labels, y, we can split our data into train, test, and evaluation sets. To do this, we'll first allocate 20% of the data into our evaluation, or holdout, set. This is data that the model never sees during training and will be used to score our trained model.\n",
        "\n",
        "We will then split the remaining data, 60/40, into train and test sets just like in supervised machine learning models. We will pass both the train and test sets into the neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sr_e_ObHq_2K"
      },
      "source": [
        "# split out evaluation sets (x_eval and y_eval)\n",
        "x_interim, x_eval, y_interim, y_eval = train_test_split(X,\n",
        "                                           y,\n",
        "                                           test_size=.2,\n",
        "                                           random_state=52)\n",
        "\n",
        "# split remaining data into train and test sets\n",
        "# ... YOUR CODE FOR TASK 5 ...\n",
        "x_train , x_test , y_train , y_test = train_test_split(x_interim,\n",
        "                                           y_interim,\n",
        "                                           test_size=.4,\n",
        "                                           random_state=52)\n",
        "# examine number of samples in train, test, and validation sets\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(len(x_train), 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "print(x_eval.shape[0], 'eval samples')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTWRGVmKrFAi"
      },
      "source": [
        "6. Model building (part i)\n",
        "It's time to start building our deep learning model, a convolutional neural network (CNN). CNNs are a specific kind of artificial neural network that is very effective for image classification because they are able to take into account the spatial coherence of the image, i.e., that pixels close to each other are often related.\n",
        "\n",
        "Building a CNN begins with specifying the model type. In our case, we'll use a Sequential model, which is a linear stack of layers. We'll then add two convolutional layers. To understand convolutional layers, imagine a flashlight being shown over the top left corner of the image and slowly sliding across all the areas of the image, moving across the image in the same way your eyes move across words on a page. Convolutional layers pass a kernel (a sliding window) over the image and perform element-wise matrix multiplication between the kernel values and the pixel values in the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlagT-6qrIT9"
      },
      "source": [
        "# set model constants\n",
        "num_classes = 1\n",
        "\n",
        "# define model as Sequential\n",
        "model = Sequential()\n",
        "\n",
        "# first convolutional layer with 32 filters\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(50, 50, 3)))\n",
        "\n",
        "# add a second 2D convolutional layer with 64 filters\n",
        "# ... YOUR CODE FOR TASK 6 ...\n",
        "model.add(Conv2D(64 , kernel_size=(3,3) ,activation='relu' ))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Yln0s8frLnz"
      },
      "source": [
        "7. Model building (part ii)\n",
        "Let's continue building our model. So far our model has two convolutional layers. However, those are not the only layers that we need to perform our task. A complete neural network architecture will have a number of other layers that are designed to play a specific role in the overall functioning of the network. Much deep learning research is about how to structure these layers into coherent systems.\n",
        "\n",
        "We'll add the following layers:\n",
        "\n",
        "MaxPooling. This passes a (2, 2) moving window over the image and downscales the image by outputting the maximum value within the window.\n",
        "Conv2D. This adds a third convolutional layer since deeper models, i.e. models with more convolutional layers, are better able to learn features from images.\n",
        "Dropout. This prevents the model from overfitting, i.e. perfectly remembering each image, by randomly setting 25% of the input units to 0 at each update during training.\n",
        "Flatten. As its name suggests, this flattens the output from the convolutional part of the CNN into a one-dimensional feature vector which can be passed into the following fully connected layers.\n",
        "Dense. Fully connected layer where every input is connected to every output (see image below).\n",
        "Dropout. Another dropout layer to safeguard against overfitting, this time with a rate of 50%.\n",
        "Dense. Final layer which calculates the probability the image is either a bumble bee or honey bee.\n",
        "To take a look at how it all stacks up, we'll print the model summary. Notice that our model has a whopping 3,669,249 paramaters. These are the different weights that the model learns through training and what are used to generate predictions on a new image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjIo3WEmrOzl",
        "outputId": "80cf98de-5516-4d99-8073-9492b3910619",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        }
      },
      "source": [
        "# reduce dimensionality through max pooling\n",
        "model.add(MaxPooling2D(pool_size = (2 , 2)))\n",
        "\n",
        "# third convolutional layer with 64 filters\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "# add dropout to prevent over fitting\n",
        "model.add(Dropout(0.25))\n",
        "# necessary flatten step preceeding dense layer\n",
        "model.add(Flatten())\n",
        "# fully connected layer\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "# add additional dropout to prevent overfitting\n",
        "# ... YOUR CODE FOR TASK 7 ...\n",
        "model.add(Dropout(0.50))\n",
        "\n",
        "# prediction layers\n",
        "model.add(Dense(num_classes, activation='sigmoid', name='preds'))\n",
        "\n",
        "# show model summary\n",
        "# ... YOUR CODE FOR TASK 7 ...\n",
        "model.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 48, 48, 32)        896       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 46, 46, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 23, 23, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 21, 21, 64)        36928     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 21, 21, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 28224)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               3612800   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "preds (Dense)                (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 3,669,249\n",
            "Trainable params: 3,669,249\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLfDr8p_rT8j"
      },
      "source": [
        "8. Compile and train model\n",
        "Now that we've specified the model architecture, we will compile the model for training. For this we need to specify the loss function (what we're trying to minimize), the optimizer (how we want to go about minimizing the loss), and the metric (how we'll judge the performance of the model).\n",
        "\n",
        "Then, we'll call .fit to begin the trainig the process.\n",
        "\n",
        "\"Neural networks are trained iteratively using optimization techniques like gradient descent. After each cycle of training, an error metric is calculated based on the difference between prediction and target…Each neuron’s coefficients (weights) are then adjusted relative to how much they contributed to the total error. This process is repeated iteratively.\" ML Cheatsheet\n",
        "\n",
        "Since training is computationally intensive, we'll do a 'mock' training to get the feel for it, using just the first 10 images in the train and test sets and training for just 5 epochs. Epochs refer to the number of iterations over the data. Typically, neural networks will train for hundreds if not thousands of epochs.\n",
        "\n",
        "Take a look at the printout for each epoch and note the loss on the train set (loss), the accuracy on the train set (acc), and loss on the test set (val_loss) and the accuracy on the test set (val_acc). We'll explore this more in a later step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WupQNrP3raoI"
      },
      "source": [
        "model.compile(\n",
        "    # set the loss as binary_crossentropy\n",
        "    loss=keras.losses.binary_crossentropy,\n",
        "    # set the optimizer as stochastic gradient descent\n",
        "    optimizer=keras.optimizers.SGD(lr=0.001),\n",
        "    # set the metric as accuracy\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# mock-train the model using the first ten observations of the train and test sets\n",
        "model.fit(\n",
        "    x_train[:10, :, :, :],\n",
        "    y_train[:10],\n",
        "    epochs=5,\n",
        "    verbose=1,\n",
        "    validation_data=(x_test[:10, :, :, :], y_test[:10])\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvKHWbySrfgf"
      },
      "source": [
        "9. Load pre-trained model and score\n",
        "Now we'll load a pre-trained model that has the architecture we specified above and was trained for 200 epochs on the full train and test sets we created above.\n",
        "\n",
        "Let's use the evaluate method to see how well the model did at classifying bumble bees and honey bees for the test and validation sets. Recall that accuracy is the number of correct predictions divided by the total number of predictions. Given that our classes are balanced, a model that predicts 1.0 for every image would get an accuracy around 0.5.\n",
        "\n",
        "Note: it may take a few seconds to load the model. Recall that our model has over 3 million parameters (weights), which are what's being loaded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_Io6cQQriiw"
      },
      "source": [
        "# load pre-trained model\n",
        "from keras.models import load_model\n",
        "pretrained_cnn = load_model('datasets/pretrained_model.h5')\n",
        "\n",
        "# evaluate model on test set\n",
        "score = pretrained_cnn.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "# evaluate model on holdout set\n",
        "eval_score = pretrained_cnn.evaluate(x_eval , y_eval , verbose = 0)\n",
        "# print loss score\n",
        "print('Eval loss:', eval_score[0])\n",
        "# print accuracy score\n",
        "print('Eval accuracy:', eval_score[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFHD0johrmGy"
      },
      "source": [
        "10. Visualize model training history\n",
        "In addition to scoring the final iteration of the pre-trained model as we just did, we can also see the evolution of scores throughout training thanks to the History object. We'll use the pickle library to load the model history and then plot it.\n",
        "\n",
        "Notice how the accuracy improves over time, eventually leveling off. Correspondingly, the loss decreases over time. Plots like these can help diagnose overfitting. If we had seen an upward curve in the validation loss as times goes on (a U shape in the plot), we'd suspect that the model was starting to memorize the test set and would not generalize well to new data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TduVHSBCro9I"
      },
      "source": [
        "# load history\n",
        "with open('datasets/model_history.pkl', 'rb') as f:\n",
        "    pretrained_cnn_history = pickle.load(f)\n",
        "\n",
        "# print keys for pretrained_cnn_history dict\n",
        "print(pretrained_cnn_history.keys())\n",
        "\n",
        "fig = plt.figure(1)\n",
        "plt.subplot(211)\n",
        "# plot the validation accuracy\n",
        "plt.plot(pretrained_cnn_history['val_acc'])\n",
        "plt.title('Validation accuracy and loss')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.subplot(212)\n",
        "# plot the validation loss\n",
        "plt.plot(pretrained_cnn_history['val_loss'], 'r')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss value');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYPalKqGrsCL"
      },
      "source": [
        "11. Generate predictions\n",
        "Previously, we calculated an overall score for our pre-trained model on the validation set. To end this notebook, let's access probabilities and class predictions for individual images using the .predict and .predict_classes methods.\n",
        "\n",
        "We now have a deep learning model that can be used to identify honey bees and bumble bees in images! The next step is to explore transfer learning, which harnesses the prediction power of models that have been trained on far more images than the mere 1600 in our dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4_TCZEsrvgY"
      },
      "source": [
        "# predicted probabilities for x_eval\n",
        "# ... YOUR CODE FOR TASK 11 ...\n",
        "\n",
        "print(\"First five probabilities:\")\n",
        "print(...)\n",
        "print(\"\")\n",
        "\n",
        "# predicted classes for x_eval\n",
        "# ... YOUR CODE FOR TASK 11 ...\n",
        "\n",
        "print(\"First five class predictions:\")\n",
        "print(y_pred[:5])\n",
        "print(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}